#!/bin/bash
# setup_local_ai.sh - Setup local AI environment

echo "ğŸš€ Setting up Local AI Environment for Vehicle Price Prediction"

# Pull Ollama Docker image
echo "ğŸ“¦ Pulling Ollama Docker image..."
docker pull ollama/ollama:latest

# Start Ollama container
echo "ğŸ³ Starting Ollama container..."
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama_data:/root/.ollama \
  ollama/ollama:latest

# Wait for Ollama to start
echo "â³ Waiting for Ollama to start..."
sleep 10

# Pull recommended models
echo "ğŸ§  Pulling recommended AI models..."
docker exec ollama ollama pull llama2:7b
docker exec ollama ollama pull codellama:7b
docker exec ollama ollama pull mistral:7b

echo "âœ… Local AI setup complete!"
echo ""
echo "ğŸ¯ Available Models:"
docker exec ollama ollama list

echo ""
echo "ğŸŒ Ollama API available at: http://localhost:11434"
echo "ğŸš— Start your Vehicle Price API with local AI support!"
echo ""
echo "ğŸ’¡ To use: Set OLLAMA_AVAILABLE=true in your environment"
